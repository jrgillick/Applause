{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from __future__ import print_function\n",
    "import glob, os, sys,re\n",
    "import numpy as np\n",
    "import spacy\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "import sys,json,gzip,random,re,math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn import linear_model\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn import cross_validation\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.contrib.rnn import BasicLSTMCell\n",
    "from tensorflow.python.ops.rnn import dynamic_rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Load in files from the Liu et al. 2017 dataset and read vocab </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "904"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_liu_transcript_paths():\n",
    "    pathname=\"/data/corpora/ted/transcripts_clean/\"\n",
    "    files = [pathname + f for f in os.listdir(pathname)]\n",
    "    talk_names_path = \"talk_names.txt\"\n",
    "    talk_names = open(talk_names_path).read().replace('.txt','.html').split('\\n')\n",
    "    talk_names = ['isabel_allende_how_to_live_passionately_no_matter_your_age.html' if t == 'isabelle_allende_how_to_live_passionately_no_matter_your_age.html' else t for t in talk_names]\n",
    "    talk_names = [t for t in talk_names if t not in ['', 'test.html', '\\r']]\n",
    "    transcripts = [pathname + t for t in talk_names]\n",
    "    return(transcripts)\n",
    "\n",
    "transcripts = get_liu_transcript_paths()\n",
    "len(transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readLIWC(filename):\n",
    "    liwc_vocab={}\n",
    "    regex_liwc={}\n",
    "    liwc={}\n",
    "    \n",
    "    # Liu excludes \"affective or emotional processes\"\n",
    "    invalid={}\n",
    "    invalid[\"31\"]=1\n",
    "    invalid[\"32\"]=1\n",
    "    invalid[\"33\"]=1\n",
    "    invalid[\"34\"]=1\n",
    "    invalid[\"35\"]=1\n",
    "\n",
    "    file=open(filename)\n",
    "    for i in range(75):\n",
    "        line=file.readline()\n",
    "        cols=re.split(\"\\s+\", line.rstrip())\n",
    "        if line.rstrip() != \"%\":\n",
    "            idd=\"%s\" % cols[0]\n",
    "            label=cols[1]\n",
    "            liwc_vocab[idd]=label\n",
    "    for line in file:\n",
    "        cols=line.rstrip().split(\"\\t\")\n",
    "        term=cols[0]\n",
    "        valid=[]\n",
    "        for x in cols[1:]:\n",
    "            if x not in invalid:\n",
    "                valid.append(x)\n",
    "        cats=[\"LIWC_%s\" % liwc_vocab[x] for x in valid]\n",
    "\n",
    "        if term.endswith(\"*\"):\n",
    "            pref=term[0:2]\n",
    "            if pref not in regex_liwc:\n",
    "                regex_liwc[pref]={}\n",
    "            regex_liwc[pref][term]=cats\n",
    "        else:\n",
    "            liwc[term]=cats\n",
    "            \n",
    "    return (liwc_vocab, regex_liwc, liwc)\n",
    "\n",
    "# get all flat LIWC categories for a word\n",
    "def getLIWC(word):\n",
    "    vals=[]\n",
    "    if word in liwc:\n",
    "        vals.extend(liwc[word])\n",
    "    if len(word) > 1:\n",
    "        pref=word[0:2]\n",
    "        if pref in regex_liwc:\n",
    "            cands=regex_liwc[pref]\n",
    "            for cand in cands:\n",
    "                if re.match(cand, word) != None:\n",
    "                    vals.extend(regex_liwc[pref][cand])\n",
    "    return vals\n",
    "\n",
    "def get_words(transcript_path):\n",
    "    words = []\n",
    "    text = open(transcript_path).read()\n",
    "    lines = text.split(\"\\n\")\n",
    "    lines = [l for l in lines if l != '']\n",
    "    paragraphs = [line.split(\"\\t\")[1] for line in lines]\n",
    "    paragraphs = [p.replace(\"(Laughter)\",\"\").replace(\"(Applause)\",\"\") for p in paragraphs]\n",
    "    for p in paragraphs:\n",
    "        words += p.split(' ')\n",
    "    return words\n",
    "    \n",
    "\n",
    "def get_vocab(transcript_path_list):\n",
    "    vocab = {}\n",
    "    \n",
    "    words = []\n",
    "    for t in transcript_path_list:\n",
    "        words += get_words(t)\n",
    "        #words += t.get_words()\n",
    "        \n",
    "    fid = 1\n",
    "    \n",
    "    for key in liwc_vocab:\n",
    "        feat=\"LIWC_%s\" % (liwc_vocab[key])\n",
    "        vocab[feat]=fid\n",
    "        fid+=1\n",
    "        \n",
    "    counts=Counter()\n",
    "    \n",
    "    for word in words:\n",
    "        counts[word.lower()]+=1\n",
    "    \n",
    "    for word in counts:\n",
    "        count=counts[word]\n",
    "        #if count >= 500:\n",
    "        if count >= 500000:\n",
    "            vocab[word.lower()]=fid\n",
    "            fid+=1\n",
    "            \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "liwc_path = '/data/corpora/LIWC/LIWC.txt'\n",
    "liwc_vocab, regex_liwc, liwc = readLIWC(liwc_path)\n",
    "vocab = get_vocab(transcripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Create \"Transcript\" object to compute features from text files </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Transcript:\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.sequences = self.get_sequences()\n",
    "        self.sentences = self.get_sentence_list()\n",
    "        \n",
    "    def get_text(self):\n",
    "        return open(self.filename).read()\n",
    "    \n",
    "    def get_split_lines(self):\n",
    "        text = self.get_text()\n",
    "        lines = text.split(\"\\n\")\n",
    "        lines = [l for l in lines if l != '']\n",
    "        return [line.split(\"\\t\") for line in lines]\n",
    "    \n",
    "    def get_paragraphs(self):\n",
    "        split_lines = self.get_split_lines()\n",
    "        return [l[1] for l in split_lines]\n",
    "        \n",
    "    def get_timestamps(self):\n",
    "        split_lines = self.get_split_lines()\n",
    "        return [l[0] for l in split_lines]\n",
    "        \n",
    "    def get_sentences(self, paragraph):\n",
    "        #par = unicode(paragraph).encode(\"utf-8\")\n",
    "        paragraph = paragraph.replace('(Laughter)','')\n",
    "        sentences = []\n",
    "        doc = nlp(paragraph)\n",
    "        for sentence in doc.sents:\n",
    "            words=[]\n",
    "            for word in sentence:\n",
    "                if re.search(\"\\S\", word.string) != None:\n",
    "                    words.append(word.string)\n",
    "            text=' '.join(words)\n",
    "            if re.match(\"^\\(.*?\\)$\", text) != None or re.search(\"\\w\", text) == None:\n",
    "                continue\n",
    "            sentences.append(text)\n",
    "        return sentences\n",
    "    \n",
    "    def get_words(self):\n",
    "        words = []\n",
    "        paragraphs = [p.replace(\"(Laughter)\",\"\").replace(\"(Applause)\",\"\") for p in self.get_paragraphs()]\n",
    "        for p in paragraphs:\n",
    "            words += p.split(' ')\n",
    "        return words\n",
    "    \n",
    "    def get_sentences_with_applause_interspersed(self, paragraph):\n",
    "        paragraph = paragraph.replace('(Laughter)','')\n",
    "        lines = paragraph.split(\"(Applause)\")\n",
    "        last_line = lines[-1]; lines = lines[0:-1]  # Don't add applause after last line\n",
    "        sentences = []\n",
    "        for line in lines:\n",
    "            sentences += self.get_sentences(line)\n",
    "            sentences.append(\"(Applause)\")\n",
    "        sentences += self.get_sentences(last_line)\n",
    "        return sentences\n",
    "      \n",
    "    def get_sequences(self):\n",
    "        sequences = []\n",
    "        split_lines = self.get_split_lines()\n",
    "        for line in split_lines:\n",
    "            if \"(Applause)\" in line[1] and line[1] != \"(Applause)\":\n",
    "                timestamp = line[0]; paragraph = line[1]\n",
    "                sentences = self.get_sentences_with_applause_interspersed(paragraph)\n",
    "                sequences.append((timestamp, paragraph, sentences))\n",
    "                #special_cases.append(line[1])\n",
    "            elif line[1] == \"(Applause)\":\n",
    "                timestamp = line[0]; paragraph = line[1]; sentences = [\"(Applause)\"]\n",
    "                sequences.append((timestamp, paragraph, sentences))\n",
    "            else:\n",
    "                timestamp = line[0]; paragraph = line[1]; sentences = self.get_sentences(paragraph)\n",
    "                sequences.append((timestamp, paragraph, sentences))\n",
    "        return sequences\n",
    "    \n",
    "    def get_sentence_list(self):\n",
    "        all_sentences = []\n",
    "        seqs = self.sequences[0:-1] #exclude last paragraph, which may be final applause\n",
    "        for sequence_index, sequence in enumerate(seqs): \n",
    "            sentences = sequence[2]\n",
    "            for sentence_index, s in enumerate(sentences):\n",
    "                if s != \"(Applause)\":\n",
    "                    features = np.concatenate((self.get_dense_liwc_features(s),self.get_word_vec_features(s)))\n",
    "                    # If this is not the last sentence in the seq, check if next sentence in this sequence is applause\n",
    "                    if sentence_index < len(sentences)-1 and sentences[sentence_index+1] == \"(Applause)\":\n",
    "                        applause_follows = 1\n",
    "                    # If this is the last sentence of this sequence, check first sentence of next sequence\n",
    "                    elif sentence_index == len(sentences) -1 and sequence_index < len(seqs) - 1:\n",
    "                        next_sentences = seqs[sequence_index+1][2]\n",
    "                        if len(next_sentences) > 0 and next_sentences[0] == \"(Applause)\":\n",
    "                            applause_follows = 1\n",
    "                        else:\n",
    "                            applause_follows = 0    \n",
    "                    else:\n",
    "                        applause_follows = 0\n",
    "                    all_sentences.append((s,features,applause_follows))\n",
    "        return all_sentences\n",
    "    \n",
    "    def count_applause_instances(self):\n",
    "        c = 0\n",
    "        sentence_list = self.sentences\n",
    "        for s in sentence_list:#[0:-1]:\n",
    "            if s[2] == 1:\n",
    "                c += 1\n",
    "        return c\n",
    "    \n",
    "    def get_applause_yes_sentences(self):\n",
    "        return [s for s in self.sentences if s[2] == 1]\n",
    "    \n",
    "    def get_applause_no_sentences(self):\n",
    "        no_sentences = [s for s in self.sentences if s[2] == 0]\n",
    "        shuffle(no_sentences)\n",
    "        return no_sentences[0:len(self.get_applause_yes_sentences())]\n",
    "    \n",
    "    def get_sparse_liwc_features(self, sentence):\n",
    "        counts = {}\n",
    "        text = sentence.lower().split(' ')\n",
    "        for word in text:\n",
    "            cats=getLIWC(word)\n",
    "            for cat in cats:\n",
    "                if vocab[cat] in counts:\n",
    "                    counts[vocab[cat]] += 1  # = 1\n",
    "                else:\n",
    "                    counts[vocab[cat]] = 1.\n",
    "            if word in vocab:\n",
    "                if vocab[word] in counts:\n",
    "                    counts[vocab[word]] += 1  # = 1\n",
    "                else:\n",
    "                    counts[vocab[word]] = 1\n",
    "        return counts\n",
    "    \n",
    "    def get_dense_liwc_features(self,sentence):\n",
    "        a = np.zeros(len(vocab))\n",
    "        sparse_feats = self.get_sparse_liwc_features(sentence)\n",
    "        for k in sparse_feats.keys():\n",
    "            a[k-1] = float(sparse_feats[k]) / len(sentence)\n",
    "        return a\n",
    "        \n",
    "    def get_word_vec_features(self,sentence):\n",
    "        return nlp(sentence).vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store objects so we don't have to recompute features \n",
    "\n",
    "import pickle\n",
    "\n",
    "def save_transcripts(filename=\"transripts.pkl\"):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(all_transcripts, f)\n",
    "\n",
    "def load_transcripts(filename=\"transcripts.pkl\"):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "transcripts_path = 'transcripts.pkl'\n",
    "\n",
    "if os.path.exists(transcripts_path):\n",
    "    all_transcripts = load_transcripts(transcripts_path)\n",
    "    \n",
    "else:\n",
    "    all_transcripts = [Transcript(f) for f in transcripts]\n",
    "    save_transcripts(transcripts_path)\n",
    "    \n",
    "all_transcripts = shuffle(all_transcripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Run Logistic Regression Models </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "from sklearn import linear_model\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn import cross_validation\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def get_balanced_applause_features(all_transcripts):\n",
    "    applause_yes = []\n",
    "    applause_no = []\n",
    "    for t in all_transcripts:\n",
    "        for a in t.get_applause_yes_sentences():\n",
    "            applause_yes.append(a[1])\n",
    "        for a in t.get_applause_no_sentences():\n",
    "            applause_no.append(a[1])\n",
    "    applause_yes = np.array(applause_yes)\n",
    "    applause_no = np.array(applause_no)\n",
    "    return applause_yes, applause_no\n",
    "\n",
    "def get_unbalanced_applause_features(all_transcripts):\n",
    "    applause_yes = []\n",
    "    applause_no = []\n",
    "    for t in all_transcripts:\n",
    "        for a in t.get_applause_yes_sentences():\n",
    "            applause_yes.append(a[1])\n",
    "        for a in t.sentences:\n",
    "            if a[2] == 0:\n",
    "                applause_no.append(a[1])\n",
    "    applause_yes = np.array(applause_yes)\n",
    "    applause_no = np.array(applause_no)\n",
    "    return applause_yes, applause_no\n",
    "\n",
    "def get_balanced_train_data_and_labels(all_transcripts):\n",
    "    applause_yes, applause_no = get_balanced_applause_features(all_transcripts)\n",
    "    X = np.vstack([applause_yes,applause_no])\n",
    "    Y = np.concatenate([np.ones(len(applause_yes)),np.zeros(len(applause_no))])\n",
    "    return X, Y\n",
    "\n",
    "def get_unbalanced_train_data_and_labels(all_transcripts):\n",
    "    applause_yes, applause_no = get_unbalanced_applause_features(all_transcripts)\n",
    "    X = np.vstack([applause_yes,applause_no])\n",
    "    Y = np.concatenate([np.ones(len(applause_yes)),np.zeros(len(applause_no))])\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Logistic Regression on balanced data following Liu et al. 2017</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accuracy: 0.722 +/- 0.014 (2826/3912)'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_true = []\n",
    "total_pred = []\n",
    "\n",
    "X,Y = get_balanced_train_data_and_labels(all_transcripts)\n",
    "X,Y = shuffle(X,Y)\n",
    "kf = KFold(len(Y), n_folds=10)\n",
    "\n",
    "for train_index, test_index in kf:\n",
    "    X_train, X_test = X[train_index][:,73:], X[test_index][:,73:]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "    \n",
    "    logreg=linear_model.LogisticRegression(penalty='l1',C=4)\n",
    "    clf = logreg\n",
    "    #clf = GridSearchCV(logreg, {'C':(0.001, .01, .1, 1, 3, 4, 5)}, cv=3)\n",
    "    #clf = GridSearchCV(logreg, {'C':(3.5,4,4.5)}, cv=3)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    #print(clf.best_params_, len(y_train))\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    for i in range(len(y_true)):\n",
    "        \n",
    "        total_true.append(y_true[i])\n",
    "        total_pred.append(y_pred[i])\n",
    "\n",
    "total_correct = np.sum(np.array(total_true) == np.array(total_pred))\n",
    "acc=float(total_correct) / len(total_true)\n",
    "std=math.sqrt( (acc * (1-acc)) / len(total_true) )\n",
    "str(\"Accuracy: %.3f +/- %.3f (%s/%s)\" % (acc, 1.96*std, total_correct, len(total_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Precision: 0.747 | Recall: 0.673 | F1: 0.708'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision, recall, f1, support = [l[1] for l in precision_recall_fscore_support(total_true, total_pred)]\n",
    "str(\"Precision: %.3f | Recall: %.3f | F1: %.3f\" % (precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Run cross validation again with clean split on full talks rather than sentences </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accuracy: 0.710 +/- 0.014 (2778/3912)'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_true = []\n",
    "total_pred = []\n",
    "\n",
    "kf = KFold(len(all_transcripts), n_folds=10)\n",
    "\n",
    "for train_index, test_index in kf:\n",
    "    t_train, t_test = np.array(all_transcripts)[train_index], np.array(all_transcripts)[test_index]\n",
    "    X_train, y_train = get_balanced_train_data_and_labels(t_train)\n",
    "    X_test, y_test = get_balanced_train_data_and_labels(t_test)\n",
    "\n",
    "    X_train, y_train = shuffle(X_train, y_train)\n",
    "    X_test, y_test = shuffle(X_test, y_test)\n",
    "\n",
    "    X_train, X_test = X_train[:,73:], X_test[:,73:]\n",
    "    \n",
    "    logreg=linear_model.LogisticRegression(penalty='l1',C=3)\n",
    "    clf = logreg\n",
    "\n",
    "    #clf = GridSearchCV(logreg, {'C':(3,3.5,4,4.5)}, cv=3)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    for i in range(len(y_true)):\n",
    "        \n",
    "        total_true.append(y_true[i])\n",
    "        total_pred.append(y_pred[i])\n",
    "\n",
    "total_correct = np.sum(np.array(total_true) == np.array(total_pred))\n",
    "acc=float(total_correct) / len(total_true)\n",
    "std=math.sqrt( (acc * (1-acc)) / len(total_true) )\n",
    "str(\"Accuracy: %.3f +/- %.3f (%s/%s)\" % (acc, 1.96*std, total_correct, len(total_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Precision: 0.732 | Recall: 0.664 | F1: 0.696'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision, recall, f1, support = [l[1] for l in precision_recall_fscore_support(total_true, total_pred)]\n",
    "str(\"Precision: %.3f | Recall: %.3f | F1: %.3f\" % (precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Train on balanced subset, test on full talks </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accuracy: 0.553 +/- 0.003 (71471/129189)'"
      ]
     },
     "execution_count": 596,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_true = []\n",
    "total_pred = []\n",
    "\n",
    "for train_index, test_index in kf:\n",
    "    t_train, t_test = np.array(all_transcripts)[train_index], np.array(all_transcripts)[test_index]\n",
    "    X_train, y_train = get_balanced_train_data_and_labels(t_train)\n",
    "    X_test, y_test = get_unbalanced_train_data_and_labels(t_test)\n",
    "\n",
    "    X_train, y_train = shuffle(X_train, y_train)\n",
    "    X_test, y_test = shuffle(X_test, y_test)\n",
    "\n",
    "    X_train, X_test = X_train[:,73:], X_test[:,73:]\n",
    "    \n",
    "    logreg=linear_model.LogisticRegression(penalty='l1')\n",
    "    clf = logreg\n",
    "\n",
    "    clf = GridSearchCV(logreg, {'C':(3,3.5,4,4.5)}, cv=3)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    for i in range(len(y_true)):\n",
    "        \n",
    "        total_true.append(y_true[i])\n",
    "        total_pred.append(y_pred[i])\n",
    "\n",
    "total_correct = np.sum(np.array(total_true) == np.array(total_pred))\n",
    "acc=float(total_correct) / len(total_true)\n",
    "std=math.sqrt( (acc * (1-acc)) / len(total_true) )\n",
    "str(\"Accuracy: %.3f +/- %.3f (%s/%s)\" % (acc, 1.96*std, total_correct, len(total_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Precision: 0.022 | Recall: 0.663 | F1: 0.043'"
      ]
     },
     "execution_count": 597,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision, recall, f1, support = [l[1] for l in precision_recall_fscore_support(total_true, total_pred)]\n",
    "str(\"Precision: %.3f | Recall: %.3f | F1: %.3f\" % (precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'C': 4.5}"
      ]
     },
     "execution_count": 598,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Train on full talks, test on full talks </h3>\n",
    "This takes a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accuracy: 0.985 +/- 0.001 (127233/129189)'"
      ]
     },
     "execution_count": 599,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_true = []\n",
    "total_pred = []\n",
    "\n",
    "for train_index, test_index in kf:\n",
    "    t_train, t_test = np.array(all_transcripts)[train_index], np.array(all_transcripts)[test_index]\n",
    "    X_train, y_train = get_unbalanced_train_data_and_labels(t_train)\n",
    "    X_test, y_test = get_unbalanced_train_data_and_labels(t_test)\n",
    "\n",
    "    X_train, y_train = shuffle(X_train, y_train)\n",
    "    X_test, y_test = shuffle(X_test, y_test)\n",
    "\n",
    "    X_train, X_test = X_train[:,73:], X_test[:,73:]\n",
    "    \n",
    "    logreg=linear_model.LogisticRegression(penalty='l1')\n",
    "    clf = logreg\n",
    "    clf = GridSearchCV(logreg, {'C':(0.001, .01, .1, 1, 3, 5)}, cv=3)\n",
    "    #clf = GridSearchCV(logreg, {'C':(3,3.5,4,4.5)}, cv=3)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    for i in range(len(y_true)):      \n",
    "        total_true.append(y_true[i])\n",
    "        total_pred.append(y_pred[i])\n",
    "\n",
    "total_correct = np.sum(np.array(total_true) == np.array(total_pred))\n",
    "acc=float(total_correct) / len(total_true)\n",
    "std=math.sqrt( (acc * (1-acc)) / len(total_true) )\n",
    "str(\"Accuracy: %.3f +/- %.3f (%s/%s)\" % (acc, 1.96*std, total_correct, len(total_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Precision: 0.000 | Recall: 0.000 | F1: 0.000'"
      ]
     },
     "execution_count": 617,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision, recall, f1, support = [l[1] for l in precision_recall_fscore_support(total_true, total_pred)]\n",
    "str(\"Precision: %.3f | Recall: %.3f | F1: %.3f\" % (precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'C': 0.001}"
      ]
     },
     "execution_count": 601,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Train LSTM over series of 5 Sentences </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_applause_indices(transcript):\n",
    "    a = [s[2] for s in transcript.sentences]\n",
    "    return list(np.nonzero(a)[0])\n",
    "\n",
    "def get_non_applause_indices(transcript):\n",
    "    return [index for index, s in enumerate(transcript.sentences) if s[2] == 0]\n",
    "\n",
    "def extract_features_from_sentence_list(l):\n",
    "    return np.array([s[1] for s in l])\n",
    "\n",
    "def extract_applause_sequences(transcript,num_sentences):\n",
    "    applause_indices = [i for i in get_applause_indices(transcript) if i > num_sentences-1]\n",
    "    sentence_lists = [transcript.sentences[i-num_sentences+1:i+1] for i in applause_indices]\n",
    "    feature_lists = [extract_features_from_sentence_list(s) for s in sentence_lists]\n",
    "    return np.array(feature_lists)\n",
    "\n",
    "def extract_non_applause_sequences(transcript,num_sentences,n):\n",
    "    non_applause_indices = [i for i in get_non_applause_indices(transcript) if i > num_sentences-1]\n",
    "    non_applause_indices = shuffle(non_applause_indices)[0:n]\n",
    "    sentence_lists = [transcript.sentences[i-num_sentences+1:i+1] for i in non_applause_indices]\n",
    "    return np.array([extract_features_from_sentence_list(s) for s in sentence_lists])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_features = 373\n",
    "n_categories = 1\n",
    "n_hidden=200\n",
    "num_sentences = 5\n",
    "number_of_layers=1\n",
    "learning_rate = 0.1\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Extract applause with a context of 4 previous sentences and train LSTM on balanced data </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_balanced_lstm_applause_features(all_transcripts, num_sentences):\n",
    "    applause_yes = None\n",
    "    applause_no = None\n",
    "    for t in all_transcripts:\n",
    "        positives = extract_applause_sequences(t,num_sentences)\n",
    "        negatives = extract_non_applause_sequences(t,num_sentences,len(positives))\n",
    "        if len(positives) != 0:\n",
    "            if applause_yes is None:\n",
    "                applause_yes = positives\n",
    "                applause_no = negatives\n",
    "            else:\n",
    "                applause_yes = np.vstack([applause_yes, positives])\n",
    "                applause_no = np.vstack([applause_no, negatives])\n",
    "    return applause_yes, applause_no\n",
    "\n",
    "def format_label(l):\n",
    "    return np.array(list(np.zeros(num_sentences-1)) + [l]).reshape((num_sentences,1))\n",
    "\n",
    "def train_tf(train_data, train_labels):\n",
    "    for i in range(len(train_data)):\n",
    "        sess.run(optimizer,feed_dict={x:train_data[i],y:train_labels[i].reshape((5,1)),seq_length:num_sentences})\n",
    "        \n",
    "def evaluate_on_data(test_data, test_labels):\n",
    "    num_correct = 0\n",
    "    for i in range(len(test_data)):\n",
    "        num_correct += sess.run(accuracy,feed_dict={x:test_data[i],y:test_labels[i].reshape((5,1)),seq_length:num_sentences})\n",
    "    return(num_correct / len(test_data))\n",
    "\n",
    "def predict_on_data(test_data):\n",
    "    return [np.round(sess.run(prediction,feed_dict={x:test_data[i],seq_length:num_sentences}))[0][0] for i in range(len(test_data))]\n",
    "\n",
    "def get_balanced_lstm_features_and_labels(all_transcripts, num_sentences):\n",
    "    applause_yes, applause_no = get_balanced_lstm_applause_features(all_transcripts, num_sentences)\n",
    "    X = np.vstack([applause_yes,applause_no])\n",
    "    Y = np.concatenate([np.ones(len(applause_yes)),np.zeros(len(applause_no))])\n",
    "    Y = np.array([format_label(l) for l in Y]).reshape((len(Y),-1))\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up tensorflow graph\n",
    "\n",
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.float32, [None, n_features])\n",
    "y = tf.placeholder(tf.float32, [None, n_categories])\n",
    "seq_length = tf.placeholder(tf.int32)\n",
    "\n",
    "weights = {'out': tf.Variable(tf.random_normal([n_hidden, n_categories])),}\n",
    "biases = {'out': tf.Variable(tf.random_normal([n_categories]))}\n",
    "\n",
    "fc1 = tf.reshape(x, [1, -1, n_features])\n",
    "lstm_fw_cell = BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "lstm_fw_multicell = tf.contrib.rnn.MultiRNNCell([lstm_fw_cell] * number_of_layers, state_is_tuple=True)\n",
    "\n",
    "output_fw, state_fw = tf.nn.dynamic_rnn(lstm_fw_multicell,fc1,dtype='float32',sequence_length=tf.reshape(seq_length, [1]))\n",
    "outputs_fw = tf.reshape(output_fw, [seq_length, n_hidden])\n",
    "out = tf.matmul(outputs_fw, weights['out']) + biases['out']\n",
    "pred = tf.gather(out,[num_sentences-1])\n",
    "end_label = tf.gather(y,[num_sentences-1])\n",
    "#end_label = tf.reshape(tf.gather(tf.reshape(y,[-1]),[num_sentences-1]),[1,1])\n",
    "\n",
    "cost = tf.nn.sigmoid_cross_entropy_with_logits(logits=pred, labels=end_label)\n",
    "cost2 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=out, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)#(learning_rate=0.1, l2_regularization_strength=0.1).minimize(cost)\n",
    "#optimizer = tf.train.FtrlOptimizer(learning_rate=learning_rate, l2_regularization_strength=1.0).minimize(cost)\n",
    "prediction = tf.sigmoid(pred)\n",
    "correct_pred = tf.equal(tf.greater_equal(prediction,0.5), tf.equal(end_label,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#kf = KFold(len(all_transcripts), n_folds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-c65a2b2eda0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mtrain_tf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_on_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_acc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-61b17e78d4e8>\u001b[0m in \u001b[0;36mtrain_tf\u001b[0;34m(train_data, train_labels)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_tf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum_sentences\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_on_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_true = []\n",
    "total_pred = []\n",
    "\n",
    "for train_index, test_index in kf:\n",
    "    t_train, t_test = np.array(all_transcripts)[train_index], np.array(all_transcripts)[test_index]\n",
    "    \n",
    "    X_train, y_train = get_balanced_lstm_features_and_labels(t_train, num_sentences=5)\n",
    "    X_test, y_test = get_balanced_lstm_features_and_labels(t_test, num_sentences=5)\n",
    "    X_train, y_train = shuffle(X_train, y_train)\n",
    "    X_test, y_test = shuffle(X_test, y_test)\n",
    "    #X_train, X_test = X_train[:,:,73:], X_test[:,:,73:]\n",
    "    \n",
    "    best_acc = 0\n",
    "    best_weights = np.zeros(n_hidden)\n",
    "    best_bias = 0.\n",
    "    val_index = int(len(X_train)*0.9)\n",
    "    val_data, val_labels = (X_train[val_index:], y_train[val_index:])\n",
    "    train_data, train_labels = (X_train[0:val_index], y_train[0:val_index])\n",
    "    \n",
    "    max_epochs = 10\n",
    "    sess.run(init)\n",
    "    for i in range(max_epochs):\n",
    "        train_data, train_labels = shuffle(train_data, train_labels)\n",
    "        train_tf(train_data, train_labels)\n",
    "        acc = evaluate_on_data(val_data, val_labels)\n",
    "        if acc > best_acc: \n",
    "            best_acc = acc\n",
    "            best_weights = sess.run(weights['out'])\n",
    "            best_bias = sess.run(biases['out'])\n",
    "        \n",
    "    assign_weights_op = tf.assign(weights['out'], best_weights)\n",
    "    assign_bias_op = tf.assign(biases['out'], best_bias)\n",
    "    sess.run(assign_weights_op)\n",
    "    y_true, y_pred = y_test[:,num_sentences-1], predict_on_data(X_test)\n",
    "    \n",
    "    for i in range(len(y_true)):      \n",
    "        total_true.append(y_true[i])\n",
    "        total_pred.append(y_pred[i])\n",
    "\n",
    "total_correct = np.sum(np.array(total_true) == np.array(total_pred))\n",
    "acc=float(total_correct) / len(total_true)\n",
    "std=math.sqrt( (acc * (1-acc)) / len(total_true) )\n",
    "str(\"Accuracy: %.3f +/- %.3f (%s/%s)\" % (acc, 1.96*std, total_correct, len(total_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "precision, recall, f1, support = [l[1] for l in precision_recall_fscore_support(total_true, total_pred)]\n",
    "str(\"Precision: %.3f | Recall: %.3f | F1: %.3f\" % (precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Implement LSTM again in Keras to compare </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Convolution2D, MaxPooling2D, Flatten, Dropout\n",
    "import keras.optimizers\n",
    "from keras.models import load_model\n",
    "import keras.regularizers\n",
    "from keras.regularizers import l2, l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "from keras.layers import regularizers\n",
    "from keras.layers import Bidirectional\n",
    "\n",
    "def initialize_lstm_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(n_hidden,input_shape=(None,373),kernel_regularizer=regularizers.l1(0.),return_sequences=False))\n",
    "    #model.add(Bidirectional(LSTM(100,  kernel_regularizer=regularizers.l2(0.0001), return_sequences=False),input_shape=(None, 300)))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    #model.add(LSTM(50,return_sequences=False))\n",
    "    #model.add(Dropout(0.5))\n",
    "    #model.add(Activation(\"relu\"))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    optimizer = keras.optimizers.Adam()\n",
    "    model.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_data_2 = np.array([t [3:,:] for t in train_data])\n",
    "#test_data_2 = np.array([t [3:,:] for t in test_data])\n",
    "#train_data_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accuracy: 0.632 +/- 0.015 (2418/3828)'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Keras model\n",
    "\n",
    "#tf.reset_default_graph()\n",
    "\n",
    "total_true = []\n",
    "total_pred = []\n",
    "\n",
    "for train_index, test_index in kf:  \n",
    "    t_train, t_test = np.array(all_transcripts)[train_index], np.array(all_transcripts)[test_index]\n",
    "    \n",
    "    X_train, y_train = get_balanced_lstm_features_and_labels(t_train, num_sentences=5)\n",
    "    X_test, y_test = get_balanced_lstm_features_and_labels(t_test, num_sentences=5)\n",
    "    X_train, y_train = shuffle(X_train, y_train)\n",
    "    X_test, y_test = shuffle(X_test, y_test)\n",
    "    \n",
    "    # Don't include labels for the whole sequence as we're only predicting at the end of the sequence\n",
    "    y_train = np.array([l[-1] for l in y_train])\n",
    "    y_test = np.array([l[-1] for l in y_test])\n",
    "    \n",
    "    best_acc = 0\n",
    "    m = initialize_lstm_model()\n",
    "    val_index = int(len(X_train)*0.9)\n",
    "    val_data, val_labels = (X_train[val_index:], y_train[val_index:])\n",
    "    train_data, train_labels = (X_train[0:val_index], y_train[0:val_index])\n",
    "    \n",
    "    max_epochs = 30; e = 0\n",
    "    while e < max_epochs:                            \n",
    "        m.fit(X_train[0:val_index], y_train[0:val_index], nb_epoch=1,batch_size=1,shuffle=True)\n",
    "        acc = m.evaluate(val_data, val_labels)[1]\n",
    "        if acc > best_acc: \n",
    "            best_acc = acc\n",
    "            m.save('best_model.h5')\n",
    "        e+=1\n",
    "        \n",
    "    m = load_model('best_model.h5')\n",
    "    \n",
    "    y_true, y_pred = y_test, np.round(m.predict(X_test))\n",
    "    for i in range(len(y_true)):\n",
    "        total_true.append(y_true[i])\n",
    "        total_pred.append(y_pred[i][0])\n",
    "    \n",
    "total_correct = np.sum(np.array(total_true) == np.array(total_pred))\n",
    "acc=float(total_correct) / len(total_true)\n",
    "std=math.sqrt( (acc * (1-acc)) / len(total_true) )\n",
    "str(\"Accuracy: %.3f +/- %.3f (%s/%s)\" % (acc, 1.96*std, total_correct, len(total_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Precision: 0.662 | Recall: 0.537 | F1: 0.593'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision, recall, f1, support = [l[1] for l in precision_recall_fscore_support(total_true, total_pred)]\n",
    "str(\"Precision: %.3f | Recall: %.3f | F1: %.3f\" % (precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
